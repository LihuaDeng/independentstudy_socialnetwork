{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import group description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as js\n",
    "with open(r\"C:\\Users\\lunad\\Desktop\\flickr\\group_info\\New folder\\ginfo_clean_ver.json\") as file_input:\n",
    "        c_ginfo = js.load(file_input)\n",
    "        group_id = list(c_ginfo.keys())\n",
    "        c_ginfo = list(c_ginfo.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and processing the texts\n",
    "import string\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ascii_chars = set(string.printable)  # speeds things up\n",
    "def remove_non_ascii_prinatble_from_list(list_of_words):\n",
    "    return [word for word in list_of_words \n",
    "            if all(char in ascii_chars for char in word)]\n",
    "\n",
    "cc_ginfo = []\n",
    "for a in range(len(c_ginfo)):\n",
    "    l = c_ginfo[a]\n",
    "    l = [i for i in l if 3 <=  len(i) <= 20]\n",
    "    l = [i for i in l if not any(c.isdigit() for c in i)]\n",
    "    l = [i.lower() for i in l if i.isalpha()]\n",
    "    l = remove_non_ascii_prinatble_from_list(l)   \n",
    "    l = [lemmatizer.lemmatize(i, 'n') for i in l]\n",
    "    l = [lemmatizer.lemmatize(i, 'v') for i in l]\n",
    "    cc_ginfo.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF Matrix and \n",
    "# Calculate Euclidean distance for each pair of group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "tfidf_matrix = vect.fit_transform(cc_ginfo)\n",
    "e_distance = euclidean_distances(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index = group_id, columns = vect.get_feature_names())\n",
    "ed_df = pd.DataFrame(e_distance, index = group_id, columns = group_id)#, index = group_id, columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the euclidean distance\n",
    "plt.imshow(e_distance, zorder=2, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "## 10 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "#X_std = StandardScaler().fit_transform(df)\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA(n_components=10)\n",
    "w_principalComponents = pca.fit_transform(tfidf_df)\n",
    "w_features = range(pca.n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(w_features, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 10 components only explained 10% variance\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first two components\n",
    "PCA_components = pd.DataFrame(w_principalComponents)\n",
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on Bag of Words\n",
    "## Building DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "data = cc_ginfo\n",
    "array = mlb.fit_transform(data)\n",
    "classes = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "bow_df = pd.DataFrame(array, index = group_id, columns = classes)\n",
    "\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "whole_X_std = StandardScaler().fit_transform(bow_df)\n",
    "# Create a PCA instance: pca\n",
    "whole_pca = PCA(n_components=10)\n",
    "w_principalComponents = whole_pca.fit_transform(whole_X_std)\n",
    "w_features = range(pca.n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(w_features, whole_pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 10 components only explained 10% variance\n",
    "whole_pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM with terms occur more then 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df.loc['Total',:]= bow_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df = bow_df.transpose()\n",
    "nbow_df = bow_df[bow_df['Total']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbow_df = nbow_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbow_df = nbow_df.drop([\"Total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "morethan1_X_std = StandardScaler().fit_transform(nbow_df)\n",
    "# Create a PCA instance: pca\n",
    "morethan1_pca = PCA(n_components=10)\n",
    "morethan1_principalComponents = morethan1_pca.fit_transform(morethan1_X_std)\n",
    "morethan1_features = range(morethan1_pca.n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(morethan1_features, morethan1_pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(morethan1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 10 components only explained 13% variance\n",
    "morethan1_pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the TF-IDF Matrix based on its score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total score for each word\n",
    "tfidf_df.loc['Total',:]= tfidf_df.sum(axis=0)\n",
    "t_df = tfidf_df.transpose()\n",
    "# Calculate the mean of the total scores of the words\n",
    "avg = Average(t_df['Total'].tolist())\n",
    "abavg_tfidf_df = t_df[t_df['Total']>=avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new tf-idf matrix with total score more than the mean\n",
    "abavg_tfidf_df = abavg_tfidf_df.transpose()\n",
    "abavg_tfidf_df = abavg_tfidf_df.drop([\"Total\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "# above_avg_X_std = StandardScaler().fit_transform(x_df)\n",
    "# Create a PCA instance: pca\n",
    "above_avg_pca = PCA(n_components=10)\n",
    "above_avg_w_principalComponents = above_avg_pca.fit_transform(abavg_tfidf_df)\n",
    "above_avg_w_features = range(above_avg_pca.n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(above_avg_w_features, above_avg_pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(above_avg_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 10 components only explained 13% variance\n",
    "above_avg_pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_PCA_components = pd.DataFrame(above_avg_w_principalComponents)\n",
    "plt.scatter(s_PCA_components[0], s_PCA_components[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "out = model.fit_transform(e_distance)\n",
    "#results = model.fit(ed_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorize = dict(c=e_distance[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))\n",
    "plt.scatter(out[:, 0], out[:, 1], **colorize)\n",
    "plt.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
