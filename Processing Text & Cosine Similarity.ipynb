{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load group data\n",
    "Here we use the group information including descriptions and rules as an example for how to do basic processing on our text data. Similar strategy can be used in processing texts in group admin profile, admin tags, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "groups = pd.read_csv(\"Nature_Groups.csv\", usecols = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = groups['groupID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get group info(i.e. description/rules) via API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oauth2\n",
    "import time\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "#Define function to call data through API\n",
    "def build_request(url, method='GET', groupid = None):\n",
    "    params = {                                            \n",
    "        'oauth_version': \"1.0\",\n",
    "        'oauth_nonce': oauth2.generate_nonce(),\n",
    "        'oauth_timestamp': int(time.time()),\n",
    "        'method': 'flickr.groups.getInfo',\n",
    "        'group_id': groupid,\n",
    "        'format': 'json',\n",
    "        'nojsoncallback':'1'     \n",
    "    }\n",
    "    consumer = oauth2.Consumer(key='cc6e0093965331473ef6d9be604b5a96', secret='a758040164b10f62')\n",
    "    token = oauth2.Token(key='72157709455713622-99a678a48136490f', secret='4a13c4e35c898f21')\n",
    "\n",
    "    params['oauth_consumer_key'] = consumer.key\n",
    "    params['oauth_token']= token.key\n",
    "\n",
    "\n",
    "    req = oauth2.Request(method=method, url=url, parameters=params)\n",
    "    signature_method = oauth2.SignatureMethod_HMAC_SHA1()\n",
    "    req.sign_request(signature_method, consumer, token)\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as js\n",
    "groupinfo = []      \n",
    "for i in group_id:     \n",
    "    print(i)                \n",
    "    each_group = []       \n",
    "    request = build_request('https://www.flickr.com/services/rest', groupid = str(i))\n",
    "    u = urllib.request.urlopen(request.to_url())\n",
    "    u = js.loads(u.read())\n",
    "    each_group.append(u)\n",
    "    groupinfo.append(each_group)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract description and rule respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look up description and rule for each group\n",
    "from nested_lookup import nested_lookup\n",
    "group_descr = {}\n",
    "group_rule = {}\n",
    "for i in range(len(groupinfo)):\n",
    "    j = groupinfo[i]\n",
    "    k = nested_lookup('description', j)\n",
    "    l = nested_lookup('rule', j)\n",
    "    group_descr[group_id[i]] = k[i]\n",
    "    group_rule[group_id[i]] = l[i]\n",
    "\n",
    "#Ourput as JSON\n",
    "description = list(group_descr.items())\n",
    "rule = list(group_rule.items())\n",
    "for u in range(len(descriptipn)):\n",
    "        t = description[u]\n",
    "        s = rule[u]\n",
    "        u_str = str(u+1)\n",
    "        d_file_name = 'DESCR_GR' + u_str + '_' + str(t[0])+ '.json'\n",
    "        r_file_name = 'RULE_GR' + u_str + '_' + str(t[0])+ '.json'\n",
    "        d = open(\"flickr/group_info/description\" + d_file_name,'w')\n",
    "        r = open(\"flickr/group_info/rule\" + d_file_name,'w')\n",
    "        js.dump(t, d)\n",
    "        js.dump(s, r)\n",
    "        d.close()\n",
    "        r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text processing\n",
    "## Group description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Processing\n",
    "#Clean texts\n",
    "from cleantext import clean\n",
    "descr = list(group_descr.values())\n",
    "clean_descr = []\n",
    "for i in range(len(descr)):\n",
    "    each_group = []\n",
    "    e_descr = descr[i]\n",
    "    clean_text = clean(e_descr,\n",
    "        fix_unicode = True,               # fix various unicode errors\n",
    "        to_ascii = True,                  # transliterate to closest ASCII representation\n",
    "        lower = True,                     # lowercase text\n",
    "        no_line_breaks = True,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls = True,                  # replace all URLs with a special token\n",
    "        no_emails = True,                # replace all email addresses with a special token\n",
    "        no_phone_numbers = True,         # replace all phone numbers with a special token\n",
    "        no_numbers = False,               # replace all numbers with a special token\n",
    "        no_digits = False,                # replace all digits with a special token\n",
    "        no_currency_symbols = True,      # replace all currency symbols with a special token\n",
    "        no_punct = True,                 # fully remove punctuation\n",
    "    #replace_with_url=\"<URL>\",\n",
    "        replace_with_email = \"<EMAIL>\",\n",
    "        replace_with_phone_number = \"<PHONE>\",\n",
    "        replace_with_number = \"<NUMBER>\",\n",
    "        replace_with_digit = \"0\",\n",
    "        replace_with_currency_symbol = \"<CUR>\",\n",
    "        lang = \"en\")                    \n",
    "    each_group.append(clean_text)\n",
    "    clean_descr.append(each_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further processing\n",
    "# Remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nostopwords_descr = []\n",
    "for i in range(len(clean_descr)):\n",
    "    each = clean_descr[i]\n",
    "    eachgroup = []\n",
    "    for j in range(len(each)):\n",
    "        k = each[j]\n",
    "        tokens = word_tokenize(k)\n",
    "        remove_stopwords = [s for s in tokens if not s in stop_words]\n",
    "        eachgroup.append(remove_stopwords)\n",
    "    nostopwords_descr.append(eachgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort words in terms of times of occurrence\n",
    "from collections import Counter\n",
    "sorted_descrwords = []\n",
    "for i in range(len(subject)):\n",
    "    j = subject[i]\n",
    "    m = Counter(item for item in j)\n",
    "    l = dict(m)\n",
    "    l_sorted = {k: v for k, v in sorted(l.items(), key=lambda x: x[1], reverse = True)}\n",
    "    sorted_descrwords.append(l_sorted)\n",
    "\n",
    "sorted_descr = {}\n",
    "for i in range(len(group_id):\n",
    "    sorted_descr[group_id[i]] = list(sorted_descrwords.keys())[i]\n",
    "\n",
    "df_descrwords = pd.DataFrame.from_dict(sorted_descr, orient='index')\n",
    "df_descrwords = df_descrwords.fillna('')\n",
    "df_descrwords = df_descrwords.transpose()\n",
    "df_descrwords.to_csv('Group_Descr_Words.csv')\n",
    "\n",
    "#Similar for processing group rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cosine similarity across groups after word embedding using pre-trained GloVe model\n",
    "Here we take the group description as an example, similar strategy can be utilized in the group rules and admin tag, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel(r\"C:\\Users\\lunad\\Downloads\\glove.6B\\glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance_wordembedding_method(s1, s2):\n",
    "    import scipy\n",
    "    vector_1 = np.mean([model[word] for word in s1],axis=0)\n",
    "    vector_2 = np.mean([model[word] for word in s2],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    percent = str(round((1-cosine)*100,2)) + \"%\"\n",
    "    return percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list od group descriptions for calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_words = group_descr.values.tolist()\n",
    "group_id = group_descr.keys.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through each pair of groups to calculate their cosine similarity on description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = []\n",
    "for a in descr_words:\n",
    "    e_d = []\n",
    "    for b in descr_words:\n",
    "        c = cosine_distance_wordembedding_method(a,b)\n",
    "        e_d.append(c)\n",
    "    cos_sim.append(e_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity_descr = pd.DataFrame(cos_sim, columns = group_id, index = group_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
