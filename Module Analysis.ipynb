{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load module data based on gephi's co-membership network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_csv(r\"C:\\Users\\lunad\\Desktop\\hai\\all_ds_co membership.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the main 9 modules\n",
    "modularity_classes = [0,1,2,3,4,5,6,7,8]\n",
    "ds = ds[ds.modularity_class.isin(modularity_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create subsets for each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_1 = ds[ds['modularity_class']== 1]['Id'].tolist()\n",
    "module_2 = ds[ds['modularity_class']== 2]['Id'].tolist()\n",
    "module_3 = ds[ds['modularity_class']== 3]['Id'].tolist()\n",
    "module_4 = ds[ds['modularity_class']== 4]['Id'].tolist()\n",
    "module_5 = ds[ds['modularity_class']== 5]['Id'].tolist()\n",
    "module_6 = ds[ds['modularity_class']== 6]['Id'].tolist()\n",
    "module_7 = ds[ds['modularity_class']== 7]['Id'].tolist()\n",
    "module_8 = ds[ds['modularity_class']== 8]['Id'].tolist()\n",
    "module_0 = ds[ds['modularity_class']== 0]['Id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load group info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as js\n",
    "with open(r\"C:\\Users\\lunad\\Desktop\\flickr\\group_info\\New folder\\ginfo_clean_ver.json\") as file_input:\n",
    "        c_ginfo = js.load(file_input)\n",
    "        group_id = list(c_ginfo.keys())\n",
    "        #c_ginfo = list(c_ginfo.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\lunad\\Desktop\\flickr\\group_info\\New folder\\sorted_ginfo.json\") as file_input:\n",
    "        sorted_ginfo = js.load(file_input)\n",
    "        group_id1 = list(sorted_ginfo.keys())\n",
    "        sorted_ginfo = list(sorted_ginfo.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign group info for each module \n",
    "ginfo_module_2 = [c_ginfo[i] for i in module_2 if i in c_ginfo]\n",
    "ginfo_module_3 = [c_ginfo[i] for i in module_3 if i in c_ginfo]\n",
    "ginfo_module_4 = [c_ginfo[i] for i in module_4 if i in c_ginfo]\n",
    "ginfo_module_5 = [c_ginfo[i] for i in module_5 if i in c_ginfo]\n",
    "ginfo_module_6 = [c_ginfo[i] for i in module_6 if i in c_ginfo]\n",
    "ginfo_module_7 = [c_ginfo[i] for i in module_7 if i in c_ginfo]\n",
    "ginfo_module_8 = [c_ginfo[i] for i in module_8 if i in c_ginfo]\n",
    "ginfo_module_0 = [c_ginfo[i] for i in module_0 if i in c_ginfo]\n",
    "ginfo_module_1 = [c_ginfo[i] for i in module_1 if i in c_ginfo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them\n",
    "ginfo_module = [ginfo_module_2,ginfo_module_1,ginfo_module_3,ginfo_module_5,ginfo_module_6,ginfo_module_7,\n",
    "             ginfo_module_8,ginfo_module_0]\n",
    "module_name = [\"module2\",\"module1\",\"module3\",\"module5\",\"module6\",\"module7\",\"module8\",\"module0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text for the group info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ascii_chars = set(string.printable)  # speeds things up\n",
    "def remove_non_ascii_prinatble_from_list(list_of_words):\n",
    "    return [word for word in list_of_words \n",
    "            if all(char in ascii_chars for char in word)]\n",
    "\n",
    "clean_ginfo_module = []\n",
    "for a in range(len(ginfo_module)):\n",
    "    l = ginfo_module[a]\n",
    "    each_group = []\n",
    "    for k in range(len(l)):\n",
    "        j = l[k]\n",
    "        j = [i for i in j if 3 <=  len(i) <= 20]\n",
    "        j = [i for i in j if not any(c.isdigit() for c in i)]\n",
    "        j = [i.lower() for i in j if i.isalpha()]\n",
    "        j = remove_non_ascii_prinatble_from_list(j)   \n",
    "        j = [lemmatizer.lemmatize(i, 'n') for i in j]\n",
    "        j = [lemmatizer.lemmatize(i, 'v') for i in j]\n",
    "        j = [i for i in j if i in model]\n",
    "        each_group.append(j)\n",
    "    clean_ginfo_module.append(each_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten lists for modules\n",
    "flat_ginfo_module = []\n",
    "for module in clean_ginfo_module:\n",
    "    flat_module = [item for sublist in module for item in sublist]\n",
    "    flat_ginfo_module.append(flat_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_ginfo_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store each module and their name\n",
    "dict_module_ginfo = {}\n",
    "for i in range(len(module_name)):\n",
    "    dict_module_ginfo[module_name[i]] = clean_ginfo_module[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrence of each word in each module\n",
    "from collections import Counter\n",
    "count = []\n",
    "for i in range(len(flat_ginfo_module)):\n",
    "    j = flat_ginfo_module[i]\n",
    "    k = Counter(item for item in j)\n",
    "    count.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the most common words (top 100) in each module\n",
    "most_common = []\n",
    "for i in range(len(count)):\n",
    "    j = count[i]\n",
    "    k = j.most_common(100)\n",
    "    most_common.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further processing the most common words for each module\n",
    "mostcommon_word = []\n",
    "for i in range(len(most_common)):\n",
    "    j = most_common[i]\n",
    "    each = []\n",
    "    for k in j:\n",
    "        m = k[0]\n",
    "        each.append(m)\n",
    "    mostcommon_word.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in a dataframe\n",
    "common_word = pd.DataFrame(mostcommon_word, index = module_name).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output as csv\n",
    "common_word.to_csv(r\"C:\\Users\\lunad\\Desktop\\module_common_words_top100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to sort the words in each module based on their frequency\n",
    "dt = []\n",
    "for i in range(len(count)):\n",
    "    j = count[i]\n",
    "    l = dict(j)\n",
    "    l_sorted = {k: v for k, v in sorted(l.items(), key=lambda x: x[1], reverse = True)}\n",
    "    dt.append(l_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get how many words are there in each module\n",
    "len_dt = []\n",
    "for i in dt:\n",
    "    l = len(i)\n",
    "    len_dt.append(l)\n",
    "    \n",
    "len_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 1% based on total number of words\n",
    "top_1per = []\n",
    "for i in len_dt:\n",
    "    l = int(i*0.01)\n",
    "    top_1per.append(l)\n",
    "top_1per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = list(dt[0].keys())[0:93]\n",
    "m1 = list(dt[1].keys())[0:30]\n",
    "m3 = list(dt[2].keys())[0:48]\n",
    "m5 = list(dt[3].keys())[0:36]\n",
    "m6 = list(dt[4].keys())[0:68]\n",
    "m7 = list(dt[5].keys())[0:46]\n",
    "m8 = list(dt[6].keys())[0:27]\n",
    "m0 = list(dt[7].keys())[0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [m2,m1,m3,m5,m6,m7,m8,m0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module_name = [\"module2\",\"module1\",\"module3\",\"module4\",\"module5\",\"module6\",\"module7\",\"module8\",\"module0\"]\n",
    "modules = [module_2, module_1, module_3, module_4, module_5, module_6, module_7, module_8, module_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10% of words in terms of their occurrence\n",
    "le = []\n",
    "for i in modules:\n",
    "    l = len(i)\n",
    "    l = int(l*0.1)\n",
    "    le.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_2 = {key: dt[0][key] for key in dt[0] if dt[0][key] >= 70}\n",
    "m_1 = {key: dt[1][key] for key in dt[1] if dt[1][key] >= 21}\n",
    "m_3 = {key: dt[2][key] for key in dt[2] if dt[2][key] >= 27}\n",
    "m_5 = {key: dt[3][key] for key in dt[3] if dt[3][key] >= 51}\n",
    "m_6 = {key: dt[4][key] for key in dt[4] if dt[4][key] >= 45}\n",
    "m_7 = {key: dt[5][key] for key in dt[5] if dt[5][key] >= 59}\n",
    "m_8 = {key: dt[6][key] for key in dt[6] if dt[6][key] >= 18}\n",
    "m_0 = {key: dt[7][key] for key in dt[7] if dt[7][key] >= 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = [m_2,m_1,m_3,m_5,m_6,m_7,m_8,m_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exxtract keys\n",
    "keys = []\n",
    "for i in range(len(mm)):\n",
    "    j = mm[i]\n",
    "    k = j.keys()\n",
    "    l = list(k)\n",
    "    keys.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM for Module 2 (Biggest one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "arrays = []\n",
    "cla = []\n",
    "for i in range(len(clean_ginfo_module)):\n",
    "    data = clean_ginfo_module[i]\n",
    "    array = mlb.fit_transform(data)\n",
    "    classes = mlb.classes_\n",
    "    arrays.append(array)\n",
    "    cla.append(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2 = pd.DataFrame(arrays[0], index = module_2, columns = cla[0])\n",
    "#df_m2 = df_m2[m2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_m1 = pd.DataFrame(arrays[1], index = module_1, columns = cla[1])\n",
    "df_m3 = pd.DataFrame(arrays[2], index = module_3, columns = cla[2])\n",
    "df_m5 = pd.DataFrame(arrays[3], index = module_5, columns = cla[3])\n",
    "df_m6 = pd.DataFrame(arrays[4], index = module_6, columns = cla[4])\n",
    "df_m7 = pd.DataFrame(arrays[5], index = module_7, columns = cla[5])\n",
    "df_m8 = pd.DataFrame(arrays[6], index = module_8, columns = cla[6])\n",
    "df_m0 = pd.DataFrame(arrays[7], index = module_0, columns = cla[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3 = df_m3[m3]\n",
    "df_m5 = df_m5[m5]\n",
    "df_m6 = df_m6[m6]\n",
    "df_m7 = df_m7[m7]\n",
    "df_m8 = df_m8[m8]\n",
    "df_m0 = df_m0[m0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2 = pd.read_csv(r\"C:\\Users\\lunad\\Desktop\\hai\\dtm_m2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TF-IDF in the DTM where document refers to different modules\n",
    "vect_m = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "tfidf_matrix_m = vect_m.fit_transform(flat_ginfo_module)\n",
    "# Calcuate Euclidean distance between modules based on the tf-idf matrix\n",
    "e_distance_m = euclidean_distances(tfidf_matrix_m)\n",
    "tf_df_m = pd.DataFrame(tfidf_matrix_m.toarray(), columns = vect_m.get_feature_names())\n",
    "e_df_m = pd.DataFrame(e_distance_m, index = module_name, columns = module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on modules: 8 to 4\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "# X_std = StandardScaler().fit_transform(df_m2)\n",
    "# Create a PCA instance: pca\n",
    "pca_m = PCA(n_components=4)\n",
    "principalComponents_m = pca_m.fit_transform(tf_df_m)\n",
    "features_m = range(pca_m.n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(features_m, pca_m.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components = pd.DataFrame(principalComponents)\n",
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TF-IDF in the DTM where document refers to each group in a module\n",
    "vect_g = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "tfidf_matrix_g = vect_g.fit_transform(clean_ginfo_module[2])\n",
    "e_distance_g = euclidean_distances(tfidf_matrix)\n",
    "tf_df_g = pd.DataFrame(tfidf_matrix_g.toarray(), columns = vect_g.get_feature_names())\n",
    "e_df_g = pd.DataFrame(e_distance_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on groups for each module\n",
    "# Take module 2 as example\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "# X_std = StandardScaler().fit_transform(df_m2)\n",
    "# Create a PCA instance: pca\n",
    "pca_m2 = PCA(n_components=10)\n",
    "principalComponents_m2 = pca_m2.fit_transform(tf_df_g)\n",
    "features_m2 = range(pca_m2.n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(features_m2, pca_m2.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "model = MDS(n_components=2, dissimilarity='euclidean', random_state=1)\n",
    "out = model.fit_transform(e_distance_m)\n",
    "results = model.fit(e_df_m.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorize = dict(c=e_distance_m[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))\n",
    "plt.scatter(out[:, 0], out[:, 1], **colorize)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = results.embedding_\n",
    "colorize = dict(c=e_distance_m[:, 0], cmap=plt.cm.get_cmap('rainbow', 8))\n",
    "plt.subplots_adjust(bottom = 0.1)\n",
    "plt.scatter(coords[:, 0], coords[:, 1], **colorize)\n",
    "\n",
    "for label, x, y in zip(e_df_m.columns, coords[:, 0], coords[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy = (x, y), \n",
    "        xytext = (-20, 20),\n",
    "        textcoords = 'offset points'\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
